{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = '''You are an experienced evaluator specializing in assessing the quality of reasoning steps in problem-solving. Your task is to find the first BAD step in a student's solution to a multiple choice question.\n",
    "\n",
    "You will judge steps as GOOD, OK or BAD based on the following criteria:\n",
    "1. GOOD Step\n",
    "A step is classified as GOOD if it meets all of these criteria:\n",
    "- Correct: Everything stated is accurate and aligns with known principles or the given problem.\n",
    "- Verifiable: The step can be verified using common knowledge, simple calculations, or a quick reference (e.g., recalling a basic theorem). If verifying requires extensive effort (e.g., detailed calculations or obscure references), mark it BAD instead.\n",
    "- Appropriate: The step fits logically within the context of the preceding steps. If a prior mistake exists, a GOOD step can correct it.\n",
    "- Insightful: The step demonstrates reasonable problem-solving direction. Even if ultimately progress in the wrong direction, it is acceptable as long as it represents a logical approach.\n",
    "\n",
    "2. OK Step\n",
    "A step is classified as OK if it is:\n",
    "- Correct and Verifiable: Contains no errors and can be verified.\n",
    "- Unnecessary or Redundant: Adds little value, such as restating prior information or providing basic encouragement (e.g., “Good job!”).\n",
    "- Partially Progressing: Makes some progress toward the solution but lacks decisive or significant advancement.\n",
    "\n",
    "3. BAD Step\n",
    "A step is classified as BAD if it:\n",
    "- Is Incorrect: Contains factual errors, misapplies concepts, derives an incorrect result, or contradicts the ground truth answer\n",
    "- Is Hard to Verify: Requires significant effort to confirm due to poor explanation.\n",
    "- Is Off-Topic: Includes irrelevant or nonsensical information.\n",
    "- Derails: Leads to dead ends, circular reasoning, or unreasonable approaches.\n",
    "\n",
    "#### Task Description\n",
    "You will be provided with:\n",
    "1. A Multiple Choice Question\n",
    "2. A Ground Truth Answer\n",
    "3. A Student's Step-by-Step Solution, where each step is enclosed with tags and indexed from 0.\n",
    "\n",
    "Once you identify a BAD step, return the index of the earliest BAD step. Otherwise,\n",
    "return the index of -1 (which denotes all steps are GOOD or OK).\n",
    "Please put your final answer (i.e., the index) in \\\\boxed{}.\n",
    "'''\n",
    "\n",
    "template = '''\n",
    "The following is a multiple choice question and its ground truth answer. You are also given a students solution (split into step, enclosed with tags and indexed from 0):\n",
    "\n",
    "[Multiple Choice Question]\n",
    "{question}\n",
    "\n",
    "[Ground Truth Answer]\n",
    "{answer}\n",
    "\n",
    "[Student Solution]\n",
    "{solution}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cot(cot):\n",
    "    solution = ''\n",
    "    for i, step in enumerate(cot):\n",
    "        solution += f'<step_{i}>\\n{step}\\n</step_{i}>\\n\\n'\n",
    "\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(question, answer, steps):\n",
    "\n",
    "\n",
    "    solution = process_cot(steps)\n",
    "\n",
    "\n",
    "    return template.format(question=question, answer=answer, solution=solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "max_gen_len = 2048\n",
    "\n",
    "with open('cot_data/mmlu_500_16/cot.json', 'r') as f:\n",
    "\n",
    "    cot_data = json.load(f)\n",
    "\n",
    "bedrock_autolabels = []\n",
    "for d in cot_data:\n",
    "    question = d['question']\n",
    "    answer = d['answer']\n",
    "    q_id = d['id']\n",
    "\n",
    "    for cot in d['chain_of_thoughts']:\n",
    "        user_prompt = get_user_prompt(question, answer, cot['steps'])\n",
    "        prompt = f'<|begin_of_text|><|start_header_id|>system<|end_header_id|>{sys_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'\n",
    "\n",
    "        cot_id = cot['cot_id']\n",
    "        bedrock_autolabels.append({'recordId': f'{q_id}+{cot_id}',\n",
    "                                   'modelInput':{'prompt':prompt,\n",
    "                                   'temperature':temp,\n",
    "                                   'max_gen_len':max_gen_len}})\n",
    "    \n",
    "\n",
    "with open('bedrock_inputs/mmlu_500_autolabel.jsonl', 'w') as f:\n",
    "\n",
    "    for d in bedrock_autolabels:\n",
    "        json.dump(d, f)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
