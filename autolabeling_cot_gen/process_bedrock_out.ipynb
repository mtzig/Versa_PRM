{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proccess json out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prm_train/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from mmlu_to_bedrock import get_mmlu\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_count = 0\n",
    "\n",
    "def extract_answer(text):\n",
    "    pattern = r\"answer is \\(?([A-J])\\)?\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # print(\"1st answer extract failed\\n\" + text)\n",
    "        \n",
    "    \n",
    "\n",
    "        return extract_again(text)\n",
    "\n",
    "\n",
    "def extract_again(text):\n",
    "    match = re.search(r'.*[aA]nswer:\\s*([A-J])', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "        return extract_final(text)\n",
    "\n",
    "\n",
    "def extract_final(text):\n",
    "    pattern = r\"\\b[A-J]\\b(?!.*\\b[A-J]\\b)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir):\n",
    "    '''\n",
    "    aggregates all jsonl file in directory into one list\n",
    "    '''\n",
    "    data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "\n",
    "        if filename.endswith('.jsonl.out'):\n",
    "\n",
    "            file_path = f'{data_dir}/{filename}'\n",
    "            with open(file_path, 'r') as f:\n",
    "                for l in f:\n",
    "                    data.append(json.loads(l))\n",
    "\n",
    "    return data\n",
    "\n",
    "def dataset_to_id_dict(ds):\n",
    "\n",
    "    id_dict = {}\n",
    "\n",
    "    for d in ds:\n",
    "        \n",
    "        # add in chain of thoughts key\n",
    "        d['chain_of_thoughts'] = []\n",
    "        id_dict[d['id']] = d\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "def get_datasets_by_id(samples=10):\n",
    "\n",
    "\n",
    "\n",
    "    ds = get_mmlu(samples=samples)\n",
    "\n",
    "\n",
    "    id_dict = dataset_to_id_dict(ds)\n",
    "\n",
    "\n",
    "    return id_dict\n",
    "\n",
    "def parse_bedrock_id(recordId):\n",
    "\n",
    "    id, cot_id = recordId.split('+')\n",
    "\n",
    "    return id, cot_id\n",
    "\n",
    "def parse_cot(cot_str, delimiter='\\n\\n'):\n",
    "    '''\n",
    "        splits cot string into list\n",
    "        parses out the generated answer\n",
    "    '''\n",
    "\n",
    "    cot_splitted = cot_str.strip().split(delimiter)\n",
    "    cot_splitted = [item.strip() for item in  cot_splitted]\n",
    "    parsed_answer = extract_answer(cot_splitted[-1])\n",
    "    return cot_splitted, parsed_answer\n",
    "\n",
    "\n",
    "def flatten_datasets_by_id(datasets_by_id, dump=True, dump_dir='.'):\n",
    "\n",
    "\n",
    "\n",
    "        datasets_by_id =list(datasets_by_id.values())\n",
    "\n",
    "        if dump:\n",
    "            with open(f'{dump_dir}/cot.json', 'w') as f:\n",
    "                json.dump(datasets_by_id, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12032/12032 [00:00<00:00, 1087204.66it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = 150\n",
    "num_gen = 64\n",
    "\n",
    "\n",
    "output_directory = f'./bedrock_outputs/mmlu_{samples}_{num_gen}'\n",
    "\n",
    "bedrock_data = get_data(output_directory)\n",
    "datasets_by_id = get_datasets_by_id(samples=samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134400/134400 [00:00<00:00, 150546.74it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fo = []\n",
    "failed_len = []\n",
    "failed_parse = []\n",
    "\n",
    "for d in tqdm(bedrock_data):\n",
    "\n",
    "    id, cot_id = parse_bedrock_id(d['recordId'])\n",
    "    # if 'chain_of_thoughts' not in datasets_by_id[dataset][split][id]:\n",
    "    #    datasets_by_id[dataset][split][id]['chain_of_thoughts'] = []\n",
    "\n",
    "    if 'modelOutput' not in d:\n",
    "        fo.append(d)\n",
    "        continue\n",
    "\n",
    "    stop_reason = d['modelOutput']['stop_reason']\n",
    "\n",
    "    # we want to ignore bad CoT\n",
    "    if stop_reason == 'length':\n",
    "        d.pop('modelOutput')\n",
    "        failed_len.append(d)\n",
    "        continue\n",
    "\n",
    "    steps, parsed_answer = parse_cot(d['modelOutput']['generation'])\n",
    "    if parsed_answer == None: # again to ignore bad CoT\n",
    "        d.pop('modelOutput')\n",
    "        failed_parse.append(d)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    datasets_by_id[id]['chain_of_thoughts'].append({'steps':steps, \n",
    "                                        'parsed_answer':parsed_answer,\n",
    "                                        'parsed_answer_correctness': parsed_answer==datasets_by_id[id]['answer'],\n",
    "                                        'cot_id':cot_id})\n",
    "    \n",
    "\n",
    "\n",
    "cot_output_path = f'cot_data/mmlu_{samples}_{num_gen}/'\n",
    "os.makedirs(os.path.dirname(cot_output_path), exist_ok=True)\n",
    "flatten_datasets_by_id(datasets_by_id, dump=True, dump_dir=cot_output_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counter = defaultdict(int)\n",
    "for d in failed_len:\n",
    "    cat = d['recordId'].split('+')[0].split('_')[-1]\n",
    "    counter[cat] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
